{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb28082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import string\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv1D, Dropout, concatenate, LSTM, RepeatVector, Dense, TimeDistributed, \\\n",
    "    LeakyReLU, BatchNormalization, AveragePooling1D, MaxPooling1D,Lambda, ReLU, Flatten, Reshape, Softmax, \\\n",
    "    Activation, Embedding\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.utils import Progbar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jiwer import wer\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"wer\")\n",
    "\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f98f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,fileModelJSON,fileWeights):\n",
    "    print(\"Saving model to disk: \",fileModelJSON,\"and\",fileWeights)\n",
    "    if Path(fileModelJSON).is_file():\n",
    "        os.remove(fileModelJSON)\n",
    "    json_string = model.to_json()\n",
    "    with open(fileModelJSON,'w' ) as f:\n",
    "        json.dump(json_string, f)\n",
    "    if Path(fileWeights).is_file():\n",
    "        os.remove(fileWeights)\n",
    "    model.save_weights(fileWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a8ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(fileModelJSON,fileWeights):\n",
    "    with open(fileModelJSON, 'r') as f:\n",
    "            model_json = json.load(f)\n",
    "            model=model_from_json(model_json)\n",
    "    model.load_weights(fileWeights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7657f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.read_csv('../data/alexa_domains.txt',names = ['url','IsMalicious'],header = None, sep = \" \")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df = df.append([df5], ignore_index=True)\n",
    "df = df.sample(frac=1, replace=True, random_state=100)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d3b2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = df.loc[df['url'].str.len() > 5]\n",
    "maxlen= 20\n",
    "X_ = df['url'].values\n",
    "tk = Tokenizer(char_level=True)\n",
    "tk.fit_on_texts(string.ascii_lowercase + string.digits + '-' + '.')\n",
    "seq = tk.texts_to_sequences(X_)\n",
    "X = sequence.pad_sequences(seq, maxlen=maxlen)\n",
    "inv_map = {v: k for k, v in tk.word_index.items()}\n",
    "X_tmp = []\n",
    "for x in X:\n",
    "    X_tmp.append(to_categorical(x,39))\n",
    "b =tk.document_count\n",
    "X = np.array(X_tmp)\n",
    "c = X[int(X.shape[0] * 0.1):, :, :]\n",
    "data_dict =  {'X_train': X[int(X.shape[0] * 0.1):, :, :],\n",
    "            \"X_test\": X[:int(X.shape[0] * 0.1), :, :],\n",
    "            \"word_index\": tk.document_count,\n",
    "            \"inv_map\": inv_map,\n",
    "            \"legit_domain\":X_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2266a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __np_sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2816e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __to_readable_domain(decoded, inv_map):\n",
    "    domains = []\n",
    "    for j in range(decoded.shape[0]):\n",
    "        word = \"\"\n",
    "        for i in range(decoded.shape[1]):\n",
    "            if decoded[j][i] != 0:\n",
    "                word = word + inv_map[decoded[j][i]]\n",
    "        domains.append(word)\n",
    "    return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae60d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    tk = Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(string.ascii_lowercase + string.digits + '-' + '.')\n",
    "    seq = tk.texts_to_sequences(df)\n",
    "    X = sequence.pad_sequences(seq, maxlen=20)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379a5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(gen_dom):\n",
    "    sampled = []\n",
    "    for x in gen_dom:\n",
    "        word = []\n",
    "        for y in x:\n",
    "            word.append(__np_sample(y))\n",
    "        sampled.append(word)\n",
    "    readable = __to_readable_domain(np.array(sampled), inv_map=data_dict['inv_map'])\n",
    "    return readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad67a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder_Input (InputLayer)      [(None, 20, 39)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "en_conv0 (Conv1D)               (None, 20, 256)      20224       Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "en_conv1 (Conv1D)               (None, 20, 256)      30208       Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "en_conv2 (Conv1D)               (None, 20, 256)      40192       Encoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 20, 256)      0           en_conv0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 20, 256)      0           en_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 20, 256)      0           en_conv2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20, 768)      0           re_lu[0][0]                      \n",
      "                                                                 re_lu_1[0][0]                    \n",
      "                                                                 re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "en_conv3 (Conv1D)               (None, 20, 8)        12296       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 20, 8)        0           en_conv3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 160)          0           re_lu_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 102,920\n",
      "Trainable params: 102,920\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def encoder_model():\n",
    "    cnn_filters = [256, 256, 256, 8]\n",
    "    cnn_kernels = [2, 3, 4, 2]\n",
    "    cnn_strides = [1, 1, 1, 1]\n",
    "    en_convs = []\n",
    "    \n",
    "    inputs = Input(shape=(20,39,),name=\"Encoder_Input\")\n",
    "    #encoder = Embedding(1000, 39,input_length=20)(inputs)\n",
    "    for i in range(3):\n",
    "        conv = Conv1D(cnn_filters[i],\n",
    "                      cnn_kernels[i],\n",
    "                      padding='same',\n",
    "                      strides=cnn_strides[i],\n",
    "                      name='en_conv%s' % i)(inputs)\n",
    "        conv = ReLU()(conv)\n",
    "        en_convs.append(conv)\n",
    "\n",
    "    encoder = concatenate(en_convs)\n",
    "    encoder = Conv1D(cnn_filters[3],\n",
    "                      cnn_kernels[3],\n",
    "                      padding='same',\n",
    "                      strides=cnn_strides[3],\n",
    "                      name='en_conv%s' % 3)(encoder)\n",
    "    encoder = ReLU()(encoder)\n",
    "    encoder = Flatten()(encoder)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=encoder, name='Encoder')\n",
    "    return model\n",
    "\n",
    "end = encoder_model()\n",
    "end.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f7e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder_Input (InputLayer)      [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 20, 8)        0           Decoder_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv0 (Conv1D)              (None, 20, 256)      4352        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv1 (Conv1D)              (None, 20, 256)      6400        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv2 (Conv1D)              (None, 20, 256)      8448        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 20, 256)      0           dec_conv0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 20, 256)      0           dec_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 20, 256)      0           dec_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 768)      0           re_lu_4[0][0]                    \n",
      "                                                                 re_lu_5[0][0]                    \n",
      "                                                                 re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv3 (Conv1D)              (None, 20, 32)       73760       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 20, 32)       0           dec_conv3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv4 (Conv1D)              (None, 20, 39)       3783        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 20, 39)       0           dec_conv4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 96,743\n",
      "Trainable params: 96,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def decoder_model(latent_vector=160):\n",
    "    cnn_filters = [256, 256, 256, 32, 39]\n",
    "    cnn_kernels = [2, 3, 4, 3, 3]\n",
    "    cnn_strides = [1, 1, 1, 1, 1]\n",
    "    dec_convs = []\n",
    "    dece =int(latent_vector/20)\n",
    "    word_index = 20\n",
    "\n",
    "    inputs = Input(shape=(latent_vector),name=\"Decoder_Input\")\n",
    "    decoder = Reshape([word_index,dece],input_shape = (latent_vector,))(inputs)\n",
    "    for i in range(3):\n",
    "        conv = Conv1D(cnn_filters[i],\n",
    "                      cnn_kernels[i],\n",
    "                      padding='same',\n",
    "                      strides=cnn_strides[i],\n",
    "                      name='dec_conv%s' % i)(decoder)\n",
    "        conv = ReLU()(conv)\n",
    "        dec_convs.append(conv)\n",
    "\n",
    "    decoder = concatenate(dec_convs)\n",
    "    decoder = Conv1D(cnn_filters[3],\n",
    "                      cnn_kernels[3],\n",
    "                      padding='same',\n",
    "                      strides=cnn_strides[3],\n",
    "                      name='dec_conv%s' % 3)(decoder)\n",
    "    decoder = ReLU()(decoder)\n",
    "    decoder = Conv1D(cnn_filters[4],\n",
    "                      cnn_kernels[4],\n",
    "                      padding='same',\n",
    "                      strides=cnn_strides[4],\n",
    "                      name='dec_conv%s' % 4)(decoder)\n",
    "    decoder = Softmax()(decoder)\n",
    "    #decoder = Flatten()(decoder)\n",
    "    #decoder = Dense(word_index)(decoder)\n",
    "    model = Model(inputs=inputs, outputs=decoder, name='Decoder')\n",
    "    return model\n",
    "\n",
    "decd = decoder_model()\n",
    "decd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a478805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 480)               10080     \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "Decoder (Functional)         (None, 20, 39)            133607    \n",
      "=================================================================\n",
      "Total params: 143,687\n",
      "Trainable params: 143,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def generator_model():\n",
    "    \"\"\"\n",
    "    Generator model:\n",
    "    param: noise vector\n",
    "    :return: generator model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(20,)))\n",
    "    model.add(Dense(480, activation='relu'))\n",
    "    model.add(ReLU())\n",
    "    model.add(decoder_model(480)) \n",
    "    return model\n",
    "              \n",
    "genr = generator_model()\n",
    "genr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3225a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder (Functional)         (None, 160)               102920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 103,081\n",
      "Trainable params: 103,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def discriminator_model():\n",
    "    \"\"\"\n",
    "    Discriminator model:\n",
    "    :return: Discriminator model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(encoder_model())\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "              \n",
    "disc = discriminator_model()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c558857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial(g, d):\n",
    "    \"\"\"\n",
    "    Adversarial Model\n",
    "    :return: Adversarial model\n",
    "    \"\"\"\n",
    "    adv_model = Sequential()\n",
    "    adv_model.add(g)\n",
    "    d.trainable = False\n",
    "    adv_model.add(d)\n",
    "    return adv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4655ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = discriminator_model()\n",
    "genr = generator_model()\n",
    "gan = adversarial(genr, disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff4b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "discr_opt = SGD(\n",
    "        lr=0.00001,\n",
    "        clipvalue=1.0,\n",
    "        decay=1e-8)\n",
    "gan_opt = Adam(\n",
    "        lr=0.000001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        decay=1e-8,\n",
    "        clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a14007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(loss='binary_crossentropy', optimizer=discr_opt, metrics=[\"accuracy\"])\n",
    "disc.trainable = True\n",
    "disc.compile(loss='binary_crossentropy', optimizer=gan_opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a1de25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder (Functional)         (None, 160)               102920    \n",
      "_________________________________________________________________\n",
      "Decoder (Functional)         (None, 20, 39)            96743     \n",
      "=================================================================\n",
      "Total params: 199,663\n",
      "Trainable params: 199,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(900000, 20, 39) (100000, 20, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgpu/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7032/7032 [==============================] - 129s 18ms/step - loss: 0.1119 - accuracy: 0.9726 - val_loss: 3.1329e-05 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "7032/7032 [==============================] - 125s 18ms/step - loss: 1.5635e-04 - accuracy: 1.0000 - val_loss: 9.9660e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "7032/7032 [==============================] - 125s 18ms/step - loss: 2.1662e-04 - accuracy: 0.9999 - val_loss: 8.7997e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "7032/7032 [==============================] - 124s 18ms/step - loss: 2.5467e-04 - accuracy: 1.0000 - val_loss: 4.2229e-04 - val_accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "7032/7032 [==============================] - 124s 18ms/step - loss: 2.3437e-04 - accuracy: 1.0000 - val_loss: 6.7402e-04 - val_accuracy: 0.9999\n",
      "3125/3125 [==============================] - 27s 8ms/step - loss: 6.7403e-04 - accuracy: 0.9999\n",
      "Loss: 0.0006740284734405577 Accuracy: 0.9999330043792725\n",
      "Saving model to disk:  ../model/GAN_Models/Autoencodermodel.json and ../model/GAN_Models/Autoencodermodel.h5\n",
      "testing\n",
      "3125/3125 [==============================] - 18s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgpu/anaconda3/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "WER: 10.000000\n",
      "['vinylpersen.nl', 'missoulian.com', 'fileden.com', 'mythoseditora.com.br', 'cotemosaique.com', 'cost1action.com', 'kolbe.com', 'innogear.net', 'freebirdgames.com', '3dprintersuperstore.com.au']\n",
      "['vinylpersen.nl', 'missoulian.com', 'fileden.com', 'mythoseditora.com.br', 'cotemosaique.com', 'cost1action.com', 'kolbe.com', 'innogear.net', 'freebirdgames.com', 'tersuperstore.com.au']\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 5\n",
    "n=20\n",
    "e = encoder_model()\n",
    "d = decoder_model()\n",
    "\n",
    "adv_model = Sequential()\n",
    "adv_model.add(e)\n",
    "adv_model.add(d)\n",
    "print(adv_model.summary())\n",
    "\n",
    "train,test = train_test_split(df, test_size=0.1)\n",
    "train_new,test_new = tokenize(train), tokenize(test)\n",
    "train_new, test_new= data_dict['X_train'],data_dict['X_test']\n",
    "print(train_new.shape,test_new.shape)\n",
    "\n",
    "\n",
    "adv_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "adv_model.fit(train_new, train_new,\n",
    "             verbose=1,\n",
    "             validation_data = (test_new, test_new),\n",
    "             batch_size=128,\n",
    "             epochs=EPOCH)\n",
    "loss, accuracy = adv_model.evaluate(test_new, test_new, verbose=1)\n",
    "print(\"Loss:\",loss,\"Accuracy:\",accuracy)\n",
    "\n",
    "model_name = \"Autoencodermodel\"\n",
    "MODEL_HOME = \"../model/GAN_Models/\"\n",
    "save_model(adv_model,MODEL_HOME + model_name + \".json\", MODEL_HOME + model_name + \".h5\")\n",
    "\n",
    "print(\"testing\")\n",
    "predictions = adv_model.predict(test_new, verbose=1)\n",
    "sampled = []\n",
    "for x in predictions:\n",
    "    word = []\n",
    "    for y in x:\n",
    "        word.append(__np_sample(y))\n",
    "    sampled.append(word)\n",
    "\n",
    "print(\"results\")\n",
    "readable = __to_readable_domain(np.array(sampled), inv_map=data_dict['inv_map'])\n",
    "dfa= df['url'].tolist()\n",
    "print(\"WER: {:2f}\".format(100 * wer(hypothesis=readable[:10], truth=dfa[:10])))\n",
    "print(dfa[:10])\n",
    "print(readable[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3118d5f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0\n",
      "Number of batches 900\n",
      "Batch size: 1000\n",
      "{0: {'loss': 0.16671010851860046, 'accuracy': 0.0}} {0: {'loss': 0.09856456518173218, 'accuracy': 0.5}}\n",
      "{100: {'loss': 0.6310011148452759, 'accuracy': 0.0}} {100: {'loss': 0.11298344284296036, 'accuracy': 0.5}}\n",
      "{200: {'loss': 0.659405529499054, 'accuracy': 0.0}} {200: {'loss': -0.1056501492857933, 'accuracy': 0.5}}\n",
      "{300: {'loss': -0.3547500669956207, 'accuracy': 0.0}} {300: {'loss': 0.039784058928489685, 'accuracy': 0.5}}\n",
      "{400: {'loss': -0.3407435417175293, 'accuracy': 0.0}} {400: {'loss': 0.0076449280604720116, 'accuracy': 0.5}}\n",
      "{500: {'loss': -0.2170599102973938, 'accuracy': 0.0}} {500: {'loss': 0.07997946441173553, 'accuracy': 0.5}}\n",
      "{600: {'loss': -0.1953703910112381, 'accuracy': 0.0}} {600: {'loss': 0.0767197534441948, 'accuracy': 0.5}}\n",
      "{700: {'loss': 0.6993337869644165, 'accuracy': 0.0}} {700: {'loss': 0.3392060399055481, 'accuracy': 0.5}}\n",
      "{800: {'loss': 0.6591306924819946, 'accuracy': 0.0}} {800: {'loss': -0.11989404261112213, 'accuracy': 0.5}}\n",
      "Epoch is 1\n",
      "Number of batches 900\n",
      "Batch size: 1000\n",
      "{0: {'loss': -0.4450928866863251, 'accuracy': 0.0}} {0: {'loss': 0.11663597822189331, 'accuracy': 0.5}}\n",
      "{100: {'loss': -0.42783990502357483, 'accuracy': 0.0}} {100: {'loss': 0.2763148248195648, 'accuracy': 0.5}}\n",
      "{200: {'loss': 0.8510988354682922, 'accuracy': 0.0}} {200: {'loss': 0.36769768595695496, 'accuracy': 0.5}}\n",
      "{300: {'loss': -0.06944667547941208, 'accuracy': 0.0}} {300: {'loss': 0.043815020471811295, 'accuracy': 0.5}}\n",
      "{400: {'loss': -0.30016088485717773, 'accuracy': 0.0}} {400: {'loss': 0.025689154863357544, 'accuracy': 0.5}}\n",
      "{500: {'loss': -0.48758262395858765, 'accuracy': 0.0}} {500: {'loss': 0.046561237424612045, 'accuracy': 0.5}}\n",
      "{600: {'loss': 0.7217838764190674, 'accuracy': 0.0}} {600: {'loss': 0.31420090794563293, 'accuracy': 0.5}}\n",
      "{700: {'loss': 0.1858588606119156, 'accuracy': 0.0}} {700: {'loss': 0.11769833415746689, 'accuracy': 0.5}}\n",
      "{800: {'loss': 0.23310549557209015, 'accuracy': 0.0}} {800: {'loss': 0.14410363137722015, 'accuracy': 0.5}}\n",
      "Epoch is 2\n",
      "Number of batches 900\n",
      "Batch size: 1000\n",
      "{0: {'loss': -0.26744532585144043, 'accuracy': 0.0}} {0: {'loss': 0.015292579308152199, 'accuracy': 0.5}}\n",
      "{100: {'loss': -0.6692377328872681, 'accuracy': 0.0}} {100: {'loss': 0.11268673837184906, 'accuracy': 0.5}}\n",
      "{200: {'loss': 0.1454986184835434, 'accuracy': 0.0}} {200: {'loss': -0.028699245303869247, 'accuracy': 0.5}}\n",
      "{300: {'loss': 0.5315523147583008, 'accuracy': 0.0}} {300: {'loss': -0.005259691271930933, 'accuracy': 0.5}}\n",
      "{400: {'loss': 1.3102335929870605, 'accuracy': 0.0}} {400: {'loss': 0.07363229244947433, 'accuracy': 0.5}}\n",
      "{500: {'loss': -0.546033501625061, 'accuracy': 0.0}} {500: {'loss': 0.31566497683525085, 'accuracy': 0.5}}\n",
      "{600: {'loss': 0.7362275123596191, 'accuracy': 0.0}} {600: {'loss': -0.012277736328542233, 'accuracy': 0.5}}\n",
      "{700: {'loss': -0.12982571125030518, 'accuracy': 0.0}} {700: {'loss': 0.02262287586927414, 'accuracy': 0.5}}\n",
      "{800: {'loss': -0.24348744750022888, 'accuracy': 0.0}} {800: {'loss': -0.08518462628126144, 'accuracy': 0.5}}\n",
      "Epoch is 3\n",
      "Number of batches 900\n",
      "Batch size: 1000\n",
      "{0: {'loss': -0.004983672872185707, 'accuracy': 0.0}} {0: {'loss': 0.060595184564590454, 'accuracy': 0.5}}\n",
      "{100: {'loss': -0.21972551941871643, 'accuracy': 0.0}} {100: {'loss': 0.0696757361292839, 'accuracy': 0.5}}\n",
      "{200: {'loss': 0.8316868543624878, 'accuracy': 0.0}} {200: {'loss': 0.048909418284893036, 'accuracy': 0.5}}\n",
      "{300: {'loss': 0.2476210743188858, 'accuracy': 0.0}} {300: {'loss': -0.16811414062976837, 'accuracy': 0.5}}\n",
      "{400: {'loss': -0.14045517146587372, 'accuracy': 0.0}} {400: {'loss': 0.24438634514808655, 'accuracy': 0.5}}\n",
      "{500: {'loss': -0.5980512499809265, 'accuracy': 0.0}} {500: {'loss': -0.006577869411557913, 'accuracy': 0.5}}\n",
      "{600: {'loss': 0.4183615744113922, 'accuracy': 0.0}} {600: {'loss': -0.11151523888111115, 'accuracy': 0.5}}\n",
      "{700: {'loss': 0.06305564194917679, 'accuracy': 0.0}} {700: {'loss': 0.012825838290154934, 'accuracy': 0.5}}\n",
      "{800: {'loss': -0.23333972692489624, 'accuracy': 0.0}} {800: {'loss': 0.2058624029159546, 'accuracy': 0.5}}\n",
      "Epoch is 4\n",
      "Number of batches 900\n",
      "Batch size: 1000\n",
      "{0: {'loss': -0.6158857345581055, 'accuracy': 0.0}} {0: {'loss': 0.22458772361278534, 'accuracy': 0.5}}\n",
      "{100: {'loss': -0.10802566260099411, 'accuracy': 0.0}} {100: {'loss': -0.16343556344509125, 'accuracy': 0.5}}\n",
      "{200: {'loss': -0.0923079401254654, 'accuracy': 0.0}} {200: {'loss': -0.23765499889850616, 'accuracy': 0.5}}\n",
      "{300: {'loss': -0.30861595273017883, 'accuracy': 0.0}} {300: {'loss': -0.0426398441195488, 'accuracy': 0.5}}\n",
      "{400: {'loss': 0.9884732961654663, 'accuracy': 0.0}} {400: {'loss': 0.110284224152565, 'accuracy': 0.5}}\n",
      "{500: {'loss': -0.3993642032146454, 'accuracy': 0.0}} {500: {'loss': 0.05207318067550659, 'accuracy': 0.5}}\n",
      "{600: {'loss': 0.3764770030975342, 'accuracy': 0.0}} {600: {'loss': 0.07844161242246628, 'accuracy': 0.5}}\n",
      "{700: {'loss': 0.9211422204971313, 'accuracy': 0.0}} {700: {'loss': -0.026506008580327034, 'accuracy': 0.5}}\n",
      "{800: {'loss': -0.2545696794986725, 'accuracy': 0.0}} {800: {'loss': -0.038330353796482086, 'accuracy': 0.5}}\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    (None, 20, 39)            143687    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 103081    \n",
      "=================================================================\n",
      "Total params: 246,768\n",
      "Trainable params: 143,687\n",
      "Non-trainable params: 103,081\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 480)               10080     \n",
      "_________________________________________________________________\n",
      "re_lu_21 (ReLU)              (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "Decoder (Functional)         (None, 20, 39)            133607    \n",
      "=================================================================\n",
      "Total params: 143,687\n",
      "Trainable params: 143,687\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder (Functional)         (None, 160)               102920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 103,081\n",
      "Trainable params: 0\n",
      "Non-trainable params: 103,081\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "train = data_dict['X_train']\n",
    "test = data_dict['X_test']\n",
    "batch = (int(train.shape[0] / BATCH_SIZE)/10)\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "        print(\"Epoch is %s\" % epoch)\n",
    "        print(\"Number of batches %s\" % int(train.shape[0] / BATCH_SIZE))\n",
    "        print(\"Batch size: %s\" % BATCH_SIZE)\n",
    "\n",
    "        for index in range(int(train.shape[0] / BATCH_SIZE)):\n",
    "            noise = np.random.normal(0, 1,size=(BATCH_SIZE, 20))\n",
    "            normal_domains = train[(index * BATCH_SIZE):(index + 1) * BATCH_SIZE]\n",
    "            \n",
    "            generated_domains = genr.predict(noise, verbose=0)\n",
    "\n",
    "            labels_size = (BATCH_SIZE, 1)\n",
    "            \n",
    "            labels_real = np.random.normal(0, 1, size=labels_size)\n",
    "            labels_fake = np.zeros(shape=labels_size)\n",
    "\n",
    "            if index % 2 == 0:\n",
    "                training_domains = normal_domains\n",
    "                labels = labels_real\n",
    "            else:\n",
    "                training_domains = generated_domains\n",
    "                labels = labels_fake\n",
    "\n",
    "            # training discriminator on both Normal and generated domains\n",
    "        \n",
    "            disc.trainable = True\n",
    "            #disc_history = disc.train_on_batch(training_domains, labels,reset_metrics=True,return_dict=True)\n",
    "            disc_history1 = disc.train_on_batch(normal_domains, labels_real,reset_metrics=True,return_dict=True)\n",
    "            disc_history2 = disc.train_on_batch(generated_domains, labels_fake,reset_metrics=True,return_dict=True)\n",
    "            disc_history = np.mean([disc_history1['loss'], disc_history2['loss']])\n",
    "            disc_acc = np.mean([disc_history1['accuracy'], disc_history2['accuracy']])\n",
    "            disc_dict = {'loss': disc_history, 'accuracy': disc_acc}\n",
    "            disc.trainable = False\n",
    "            \n",
    "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, 20))  # random latent vectors.\n",
    "            misleading_targets = np.random.normal(0, 1, size=labels_size)\n",
    "            gan_history = gan.train_on_batch(noise, misleading_targets,reset_metrics=True,return_dict=True) \n",
    "            if (index % 100 == 0):\n",
    "                print({index:gan_history},{index:disc_dict})\n",
    "print(gan.summary())\n",
    "print(genr.summary())\n",
    "print(disc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "869b5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to disk:  ../model/GAN_Models/Generatormodel.json and ../model/GAN_Models/Generatormodel.h5\n",
      "Saving model to disk:  ../model/GAN_Models/Discriminatormodel.json and ../model/GAN_Models/Discriminatormodel.h5\n",
      "Saving model to disk:  ../model/GAN_Models/GANmodel.json and ../model/GAN_Models/GANmodel.h5\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Generatormodel\"\n",
    "MODEL_HOME = \"../model/GAN_Models/\"\n",
    "save_model(genr,MODEL_HOME + model_name + \".json\", MODEL_HOME + model_name + \".h5\")\n",
    "\n",
    "model_name = \"Discriminatormodel\"\n",
    "MODEL_HOME = \"../model/GAN_Models/\"\n",
    "save_model(disc,MODEL_HOME + model_name + \".json\", MODEL_HOME + model_name + \".h5\")\n",
    "\n",
    "model_name = \"GANmodel\"\n",
    "MODEL_HOME = \"../model/GAN_Models/\"\n",
    "save_model(gan,MODEL_HOME + model_name + \".json\", MODEL_HOME + model_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2345148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgpu/anaconda3/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['startupily.com',\n",
       " 'aznoticias.mx',\n",
       " 'angelhotel.com.tw',\n",
       " 'mwt.ru',\n",
       " 'hku-szh.org',\n",
       " 'goodmart.com',\n",
       " 'irobot-jp.com',\n",
       " 'ruhraktuell.com',\n",
       " 'raz.ru',\n",
       " 'asanatlar.com']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled = []\n",
    "for x in normal_domains:\n",
    "    word = []\n",
    "    for y in x:\n",
    "        word.append(__np_sample(y))\n",
    "    sampled.append(word)\n",
    "\n",
    "print(\"results\")\n",
    "readablen = __to_readable_domain(np.array(sampled), inv_map=data_dict['inv_map'])\n",
    "readablen[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2245aa0",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['igj5wyyyzmy2zqfsh65v',\n",
       " 'beta45yaipv4q6q3m3on',\n",
       " 'jo601dpse76aij3ii.x5',\n",
       " 'n4sb5zpbggy38gys7p9',\n",
       " '852zir35niey2ayh1i2u',\n",
       " '-4uhvh8rmwwaehcu9w7l',\n",
       " '7a4obtg81sy51lde0v38',\n",
       " 't4v6752tndh6e9arrrc',\n",
       " 'h6vs879jyfqxayyynuz',\n",
       " 'ablf7b3pkz6ju1ab1lwb']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled = []\n",
    "for x in generated_domains:\n",
    "    word = []\n",
    "    for y in x:\n",
    "        word.append(__np_sample(y))\n",
    "    sampled.append(word)\n",
    "\n",
    "print(\"results\")\n",
    "readableg = __to_readable_domain(np.array(sampled), inv_map=data_dict['inv_map'])\n",
    "readableg[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}